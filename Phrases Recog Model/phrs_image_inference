import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf

# Load the TensorFlow Lite model
TFLITE_MODEL_PATH = 'tensorflow_model.tflite'
interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Define the actions (gestures)
actions = ['hello', 'thanks', 'iloveyou', 'sorry']

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

def extract_hand_landmarks(image, model):
    """Extract hand landmarks from an image using MediaPipe."""
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB
    image.flags.writeable = False  # Make image non-writeable for efficiency
    results = model.process(image)  # Process the image
    image.flags.writeable = True  # Make image writeable again

    # Extract landmarks for the first detected hand
    if results.multi_hand_landmarks:
        hand_landmarks = results.multi_hand_landmarks[0]
        return np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()
    else:
        return np.zeros(21 * 3)  # Return zero-filled array if no hand is detected

# Start video capture
cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Flip the frame horizontally for a mirror effect
        frame = cv2.flip(frame, 1)

        # Extract hand landmarks
        landmarks = extract_hand_landmarks(frame, hands)

        # Predict the gesture if landmarks are detected
        if np.any(landmarks):  # Check if landmarks are not all zeros
            # Preprocess the input for TensorFlow Lite
            input_data = np.expand_dims(landmarks.astype(np.float32), axis=0)

            # Set the input tensor
            interpreter.set_tensor(input_details[0]['index'], input_data)

            # Run inference
            interpreter.invoke()

            # Get the output tensor
            output_data = interpreter.get_tensor(output_details[0]['index'])
            prediction = np.argmax(output_data)  # Get the class with the highest probability
            confidence = output_data[0][prediction]  # Get the confidence level
            predicted_action = f"{actions[prediction]} ({confidence * 100:.2f}%)"
        else:
            predicted_action = "No hand detected"

        # Draw landmarks on the frame
        results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # Display the predicted action and confidence level
        cv2.putText(frame, f"Action: {predicted_action}", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Show the frame
        cv2.imshow('Real-Time Gesture Recognition', frame)

        # Exit on 'Q' key
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()